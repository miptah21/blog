{"AI/AIChatbot":{"slug":"AI/AIChatbot","filePath":"AI/AIChatbot.md","title":"AIChatbot","links":[],"tags":[],"content":"My AI Experiment\nHere is my AI chatbot:\n\n"},"AI/Overview":{"slug":"AI/Overview","filePath":"AI/Overview.md","title":"Overview","links":[],"tags":[],"content":"AI engineering refers to the process of building applications on top of foundation\nmodels.\nIf traditional ML engineering involves developing ML models, AI engineering lever‐\nages existing ones. The availability and accessibility of powerful foundation models\nlead to three factors that, together, create ideal conditions for the rapid growth of AI\nengineering as a discipline:\nFactor 1: General-purpose AI capabilities\nFactor 2: Increased AI investments\n\nAccording to WallStreetZen, companies that mentioned AI in their earning calls saw their stock price increase more than those that didn’t: an average of a 4.6% increase compared to 2.4%. It’s unclear whether it’s causation (AI makes these companies more successful) or correlation (companies are successful because they are quick to adapt to new technologies).\n\nFactor 3: Low entrance barrier to building AI applications"},"DSA/Introduction-to-DSA":{"slug":"DSA/Introduction-to-DSA","filePath":"DSA/Introduction to DSA.md","title":"Introduction to DSA","links":[],"tags":[],"content":"\n“How many different ways can I mess it up?”\n\nTrees\nBinary Trees\n\nhave 2 pointer\nit almost similar to double linked list but we typically draw a pointer down, because there a relationship between nodes, (parent and child)\nleaf nodes = nodes that don’t have any children or bottom nodes\nthe root nodes = the top of all the nodes or only have single nodes root\nwe can’t have cycles in binary trees or the pointer of leaf nodes cannot point into root nodes, the one that can have cycles iis linked list.\nsibling nodes = the nodes that have same parent\nDescendant = all nodes under the original nodes / Ancestor = all nodes above of the original nodes\nHeight = if you have 1 desc and 1 ancestor you can count 2 nodes is the height from original nodes. (ex: the height  of nodes 1 is 2 “nodes 1 and 4”)\nin some case people called the height of single nodes is zero, and for the height of two nodes is 1, and ect. but we don’t prefer to count this way.\ndepth = is same as height but you count from down.\n\n\nExample\nclass TreeNode {\n\tint val;\n\tTreeNode left = null;\n\tTreeNode right = null;\n \n\tTreeNode(int val) {\n\t\tthis.val = val;\n\t}\n}\n\nBinary Search Trees (BST)\n\nSorted\nLeft tree, the node is less than the root value\nRight tree, the node more greater than the root value\nabove sorted property is true for every single nodes\nDO not  contains duplicate\nTrue = Found the target, False =  not found, that if you perform search\nusing recursive in nature, meaning sub tree has exactly structure with entire tree\nTime Complexity = h(height of trees) or O(log(n)) → only if we get binary tree balance, meaning that the high left and right of sub tree is equal or maybe differ by only one, if not balance the time complexity = O(n) or most people will say O(h) → height\ninserting/deleting value in BST can also be log(n)\n\n\nExample:\n\tdef search(root, target):\n\t\tif not root:\n\t\t\treturn False\n\t\t\n\t\tif target &gt; root.val:\n\t\t\treturn search(root.right, target)\n\t\tif target &lt; root.val:\n\t\t\treturn search(rot.left, target)\n\t\telse:\n\t\t\treturn True\n\nBinary Search Trees (BST) - Insert and Remove\nInsert\n\ninsert/remove = log(n) time, assuming trees is roughly balance\ninsert nodes at leaf nodes more easy but will cause not balance\ntime complexity is the height of the trees, if balance = O(log(n))\n\n\nExample:\ndef insert(root, val):\n\tif not root:\n\t\treturn TreeNode(val)\n \n\tif val &gt; root.val:\n\t\troot.right = insert(root.right, val)\n\telif val &lt; root.val:\n\t\troot.left = insert(root.left, val)\n\treturn root\nRemove\n\nFind min of the trees\nRemove nodes have 2 cases:\n\nnodes that have 0 or 1 child (more simple)\nnodes that have 2 children\n\n\nif remove root nodes, replaces it with leaf nodes / the smallest value, in the right sub trees\nor you can replace it with the largest value from the left sub trees\n\n\nExample:\n# FInd Min fo the trees\n# no need recursive func because we go one direction only left\ndef minValueNode(root):\n\tcurr = root\n\twhile curr and curr.left:\n\t\tcurr = curr.left\n\treturn curr\n \n#Remove func\ndef remove(root, val):\n\tif not root:\n\t\treturn None\n \n\tif val &gt; root.val:\n\t\troot.right = remove(root.right, val)\n\telif val &lt; root.val:\n\t\troot.left = remove(root.left, val)\n\telse:\n\t\tif not root.left:\n\t\t\treturn root.right\n\t\telif not root.right:\n\t\t\treturn root.left\n\t\telse:\n\t\t\tminNode = minValueNode(root.right)\n\t\t\troot.val = minNode.val\n\t\t\troot.right = remove(root.right, minNode.val)\n\treturn root\n\nBST - Traversal\n\nIn-order traversal\nLeft → Root → Right\nTime complexity\n\nworst case = O(n) → traves entire trees\nbuilding the entire trees itself = O(nlog(n))\ntraves it/building output array = O(n)\nso time complexity = O(n + nlogn) ⇒ nly cares the larger = O(nlogn)\nso to sorting random array using bst-traversal need time comlexity = O(nlogn)\n\n\n\n\nExample:\ndef inorder(root):\n\tif not root:\n\t\treturn\n\tinorder(root.left)\n\tprint(root.val)\n\tinorder(root.right)\n\nPre-Order Traversal\nRoot → Left → Right\n\nExample:\ndef preorder(root):\n\tif not root:\n\t\treturn\n\tprint(root.val)\n\tpreorder(root.left)\n\tpreorder(root.right)\n\nPost-Order Traversal\nLeft  → Right → Root\n\nExample:\ndef postorder(root):\n\tif not root:\n\t\treturn\n\tpostorder(root.left)\n\tpostorder(root.right)\n\tprint(root.val)\n\nReverse-Order Traversal\nRight → Root → Left\n\nExample:\ndef reverseorder(root):\n\tif not root:\n\t\treturn\n\treverseorder(root.right)\n\tprint(root.val)\n\treverseorder(root.left)\nDepth-First Search (DFS)\n\nthe most common algorithm in general\nall bst traversal above are example of DFS\nas the name if we search is go with depth first / go depth as we can first / reach the bottom of the trees\n\n\nBreadth-First-Search (BFS)\n\nthe second most common algorithm in general\ntravers trees layer by layer\nor going with the closest node first\nits called level-order traversal\nfirst from top/root we process the nodes → the children of the nodes from the left to the right  → and then iterate that process until bottom trees\nQueue = the data structure that  we use, add the element first in first out\nTime complexity = O(n)\n\n\nExample:\nfrom collections import deque\n \ndef bfs(root):\n\tqueue = deque()\n \n\tif root:\n\t\tqueue.append(root)\n \n\tlevel = 0\n\twhile len(queue) &gt; 0:\n\t\tprint(&quot;level: &quot;,level)\n\t\tfor i in range(len(queue)):\n\t\t\tcurr = queue.popleft()\n\t\t\tprint(curr.val)\n\t\t\tif curr.left:\n\t\t\t\tqueue.append(curr.left)\n\t\t\tif curr.right:\n\t\t\t\tqueue.append(curr.right)\n\t\tlevel += 1\n\nBST - Sets and Maps\n\nif use sets in bst, instead of array\nthe time complexity = O(logn) → search, insert, remove\ntypically called order sets or trees sets\n\n# Sets\n{1, 2, 3}\n\ntypically called order maps or trees maps\n\n# Maps\n# ex: PhoneBook\nkey       value\n---       ---\nalice  -&gt; 123\nbrian  -&gt; 456\ncollin -&gt; 789\n \n# [Key and Value]\n-&gt; key is how we sorted value or vice versa\nExample:\n# Python\nfrom sortedcontainers import SortedDict\n \ntreeMap = SortedDict({&#039;c&#039;: 3, &#039;a&#039;: 1, &#039;b&#039;:2})\n// JavaScript\nconst TreeMap = require(&quot;treemap-js&quot;);\n \nvar map = new TreeMap();\n// C++\nmap&lt;string, string&gt; treeMap;\n// Java\nTreeMap&lt;String, String&gt; tree_map\n\t= new TreeMap&lt;String, String&gt;();\n\nBacktracking\nTree Maze\n\nBacktracking algorithm → base on DFS but not in\nthe similar ide of backtracking is like going to a maze\nbasically we recursively we try every single path\nits like brute force approach but basically we try go through every single possibility\nBacktracking = can’t use advantages of sorted property like BST\nwe go with binary trees not BST and we check both the left and the right sub trees\nif we already find the right path we don’t go all the rest sub trees\nbacktracking can use more than just binary trees\nTime complexity:\n\nWorst case = O(n) → travers entire trees / runs all possibility\n\n\n\n\nExample:\ndef canReachLeaf(root):\n\tif not root or root.val == 0:\n\t\treturn False\n \n\tif not root.left and not root.right:\n\t\treturn True\n\tif canReachLeaf(root.left):\n\t\treturn True\n\tif canReachLeaf(root.right):\n\t\treturn True\n\treturn False\nInstead return True or False, lets just return what the value of the path\n\nExample:\ndef leafPath(root, path):\n\tif not root or root.val == 0:\n\t\treturn False\n\tpath.append(root.val)\n \n\tif not root.left and not root.right:\n\t\treturn True\n\tif leafPath(root.left, path):\n\t\treturn True\n\tif leafPath(root.right, path):\n\t\treturn True\n\tpath.pop()\n\treturn False\n\nHeap Priority Queue\nHeap Properties\n\n\nQueue = FIFO (first value in first value out)\n\n\nwe can order in some types of priority values\n\n\nthe priority is variety, based on (Min/Max)\n\n\nHeap/Priority Queue = interface with priority queue, but under the hood implemented using heap.\n\n\nMin is most common use in heap\n\n\n\nBinary Heap is essentially tress that is consider complete Binary Trees. or means we have a trees that every single level of the trees there is no holes or null expect thelast level of the trees or leaf nodes.\n\n"},"MLOps/1.-Introduction-to-Machine-Learning-in-Production/Overview":{"slug":"MLOps/1.-Introduction-to-Machine-Learning-in-Production/Overview","filePath":"MLOps/1. Introduction to Machine Learning in Production/Overview.md","title":"Overview","links":[],"tags":[],"content":"Lihat Dokumen\nBerikut dokumen bisa dibaca langsung di halaman:\n\n\n"},"MLOps/1.-Introduction-to-Machine-Learning-in-Production/Week-1---Overview-of-ML-Lifecycle-and-Deployment":{"slug":"MLOps/1.-Introduction-to-Machine-Learning-in-Production/Week-1---Overview-of-ML-Lifecycle-and-Deployment","filePath":"MLOps/1. Introduction to Machine Learning in Production/Week 1 - Overview of ML Lifecycle and Deployment.md","title":"Week 1 - Overview of ML Lifecycle and Deployment","links":[],"tags":[],"content":"Dokumen ini memberikan overview ML lifecycle dan deployment, serta menekankan pentingnya MLOps untuk membangun karier AI yang efektif.\nTakeaways Utama\n1. Pentingnya MLOps\n\nKarier AI yang efektif membutuhkan:\n\nKonsep machine learning / deep learning\nKemampuan production engineering\n\n\nMenggabungkan fondasi ML dengan software development modern.\n\n2. Lifecycle Proyek ML\nEmpat tahap utama:\n\nScoping\n\nMendefinisikan proyek, metrik, dan sumber daya.\n\n\nData\n\nDefinisi, labeling, organisasi data.\nMembuat baseline performa.\n\n\nModeling\n\nMemilih dan melatih model.\nAnalisis error untuk perbaikan.\n\n\nDeployment\n\nMen-deploy, memonitor, dan maintain sistem di production.\n\n\n\n3. Gap POC ke Production\n\nModel di Jupyter Notebook hanya mewakili 5-10% kode total proyek ML.\nDeployment memerlukan integrasi software yang lengkap agar model bisa berjalan di dunia nyata.\n\n4. Tantangan Deployment\na. ML / Statistical Issues\n\nConcept drift: perubahan definisi Y given X.\nData drift: perubahan distribusi X.\nBisa menurunkan performa model setelah deployment.\n\nb. Software Engineering Issues\n\nKeputusan penting:\n\nReal-time vs batch prediction\nCloud vs edge/browser deployment\nSumber daya: CPU/GPU/memori\nLatency &amp; throughput\nLogging, security, privacy\n\n\n\n5. Deployment Patterns\n\nNew Product / Capability: gradual ramp-up + monitoring.\nAutomate / Assist Manual Task: shadow mode deployment (ML berjalan paralel, output tidak digunakan untuk keputusan).\nReplace Previous ML System: gradual ramp-up + monitoring + rollback.\nCanary Deployment: deploy ke sebagian kecil traffic, monitoring, lalu naikkan traffic.\nBlue-Green Deployment: old (blue) &amp; new (green) version, switch traffic, mudah rollback.\n\n6. Derajat Otomasi\nDeployment adalah spektrum:\n\nHuman-only\nShadow mode / AI assistance\nPartial automation (human-in-the-loop untuk prediksi tidak pasti)\nFull automation\n\n7. Monitoring\n\nPenting untuk tracking health dan performance sistem.\nLangkah-langkah:\n\nBrainstorm masalah &amp; metrik terkait.\nMonitor:\n\nSoftware metrics: memory, compute, latency, throughput, server load\nInput metrics: avg input length, missing values, brightness\nOutput metrics: null output, user redo search, switch to typing\n\n\nSetting threshold &amp; adaptasi metrik seiring waktu\n\n\nModeling &amp; deployment adalah proses iteratif.\n\n8. Model Maintenance\n\nRetraining bisa manual atau otomatis (manual lebih umum).\nMonitoring membantu:\n\nMenentukan kapan retraining dibutuhkan\nAnalisis error lebih lanjut\n\n\n\n9. Pipeline Monitoring\n\nSistem AI kompleks dengan banyak komponen membutuhkan monitoring di tiap stage pipeline.\nPerubahan di satu modul bisa memengaruhi modul lainnya.\n\n10. Data Change Rate\n\nData user lebih lambat drift\nData enterprise/B2B lebih cepat berubah\n\n\n\nKesimpulan: ML production bukan hanya soal model, tapi integrasi end-to-end, monitoring, maintenance, dan deployment pattern yang tepat. MLOps adalah skill kunci untuk membuat ML bisa memberi nilai bisnis nyata.\n\nLihat Dokumen\nBerikut dokumen bisa dibaca langsung di halaman:\n\n"},"MLOps/1.-Introduction-to-Machine-Learning-in-Production/Week-2---Select-and-Train-a-Model":{"slug":"MLOps/1.-Introduction-to-Machine-Learning-in-Production/Week-2---Select-and-Train-a-Model","filePath":"MLOps/1. Introduction to Machine Learning in Production/Week 2 - Select and Train a Model.md","title":"Week 2 - Select and Train a Model","links":[],"tags":[],"content":"Dokumen ini membahas pemilihan dan pelatihan model ML, dengan fokus pada iterasi, data-centric AI, dan manajemen eksperimentasi.\n\n1. Modelling Overview &amp; Key Challenges\n\nML lifecycle: iterasi antara model, hyperparameter, dan data.\nData-centric AI vs Model-centric AI:\n\nKualitas data sangat penting.\nMeningkatkan data sering lebih efektif daripada fokus hanya pada arsitektur model.\n\n\nAI system = kode + data.\n\n\n2. Iterative Model Development\n\nProses pengembangan model bersifat iteratif:\n\nTrain model\nAnalisis error\nAudit performa\nPerbaikan data &amp; hyperparameter\n\n\n\n\n3. Low Average Error Isn’t Enough\n\nContoh penting:\n\nNavigational queries di search engine\nSlice data kritikal: loan approval, rekomendasi produk\n\n\nPerformance harus sempurna pada subset penting, meskipun jarang muncul.\n\n\n4. Rare Classes &amp; Skewed Datasets\n\nAkurasi saja tidak cukup untuk dataset tidak seimbang.\nGunakan metrik:\n\nConfusion matrix\nPrecision / Recall\nF1 score\n\n\nBerlaku juga untuk multi-class problems dengan rare classes.\n\n\n5. Establish a Baseline\n\nPenting menetapkan baseline performa di awal proyek:\n\nHuman-level performance (HLP) untuk data tidak terstruktur\nState-of-the-art / open source\nQuick-and-dirty implementation\nPerformance sistem lama\n\n\n\n\n6. Tips Memulai Modelling\n\nLakukan literature search, tapi jangan obses.\nGunakan algoritma dengan open-source implementation yang wajar.\nPertimbangkan deployment constraints setelah baseline.\nSanity-check code: coba overfit dataset kecil.\n\n\n7. Error Analysis\n\nCore dari proses ML development.\nLangkah-langkah:\n\nPeriksa mislabeled examples\nTag error (misal: car noise, blurry image)\nMetrics: fraction of errors per tag, misclassification rate, frequency tag\n\n\nPrioritaskan perbaikan berdasarkan:\n\nGap ke HLP\nFrekuensi kategori\nKemudahan meningkatkan akurasi\nBusiness importance\n\n\n\n\n8. Adding / Improving Data\n\nKumpulkan lebih banyak data untuk kategori yang butuh improvement.\nGunakan data augmentation untuk unstructured data:\n\nRealistis\nMapping jelas x → y\nAlgoritma saat ini performa buruk\n\n\nPerhatikan resiko menambah data terutama untuk model besar.\n\n\n9. Adding Features (Structured Data)\n\nUntuk structured data sulit membuat contoh baru:\n\nTambahkan fitur baru yang relevan.\nContoh: fitur vegetarian preference untuk rekomendasi restoran.\n\n\n\n\n10. Experiment Tracking\n\nPenting untuk mengelola banyak eksperimen:\n\nInformasi yang perlu dicatat: algoritma/code version, dataset, hyperparameters, metrics, trained model\n\n\nTools:\n\nText file / spreadsheet\nWeights &amp; Biases, Comet, MLflow\n\n\n\n\n11. From Big Data to Good Data\n\nData berkualitas tinggi konsisten sepanjang lifecycle ML:\n\nCover kasus penting\nDefinisi konsisten\nFeedback tepat waktu dari production\nUkuran data sesuai kebutuhan\n\n\n\n\n\nKesimpulan: Keberhasilan ML bukan hanya soal arsitektur model, tapi data berkualitas + error analysis + iterative improvement + tracking eksperimen.\n\nLihat Dokumen\nBerikut dokumen bisa dibaca langsung di halaman:\n\n"},"MLOps/1.-Introduction-to-Machine-Learning-in-Production/Week-3---Data-Definition-and-Baseline":{"slug":"MLOps/1.-Introduction-to-Machine-Learning-in-Production/Week-3---Data-Definition-and-Baseline","filePath":"MLOps/1. Introduction to Machine Learning in Production/Week 3 - Data Definition and Baseline.md","title":"Week 3 - Data Definition and Baseline","links":[],"tags":[],"content":"Dokumen ini membahas definisi data dan menetapkan baseline performa untuk proyek ML, dengan fokus pada kualitas data, human-level performance, dan scoping proyek.\n\n1. Mengapa Data Definition Sulit\n\nLabeling tidak konsisten → algoritma bingung.\nContoh: deteksi iguana, defect phone, speech recognition.\nInstruksi labeling ambigu → interpretasi berbeda antar labeler.\n\n\n2. Jenis Masalah Data\nDikelompokkan menurut data type &amp; jumlah data:\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nData TypeSmall Data (≤10k)Big Data (&gt;10k)UnstructuredLabel harus bersih &amp; konsisten, bisa direview manualFokus pada proses data &amp; instruksi konsisten untuk tim besarStructuredData sulit ditambah, human labeling terbatasFokus pada proses data &amp; standardisasi\n\nUnstructured: manusia cukup baik, bisa pakai data augmentation.\nStructured: sulit menambah data, labeling manual tidak selalu konsisten.\n\n\n3. Meningkatkan Konsistensi Label\n\nBeberapa labeler memberi anotasi pada contoh yang sama.\nDiskusi antara ML engineer, expert, dan labeler untuk menyepakati definisi.\nJika input (x) kurang jelas → perbaiki (misal lighting kamera).\nIterasi instruksi labeling sampai kesepakatan meningkat.\nStrategi:\n\nStandarisasi label\nGabungkan kelas ambigu\nTambahkan kelas “borderline/unintelligible” untuk contoh tidak pasti\n\n\n\n\n4. Human Level Performance (HLP)\n\nKenapa diukur: estimasi Bayes error, prioritas error, target realistis untuk bisnis.\nTantangan: HLP bisa memberi algoritma “keuntungan” jika instruksi labeling ambigu.\nMeningkatkan HLP: perbaiki konsistensi labeling → data lebih bersih → ML lebih baik.\n\n\n5. Mendapatkan Data\n\nTime investment: fokus iterasi cepat (model, hyperparameter, data, training, error analysis).\nInventory data sources: milik sendiri, crowdsourced, paid, purchased. Pertimbangkan jumlah, biaya, kualitas, privasi, regulasi.\nLabeling options: in-house, outsourced, crowdsourced.\n\nJangan langsung menambah data &gt;10x.\nSME penting untuk tugas spesialisasi.\nML engineers labeling beberapa hari → membangun intuisi.\n\n\n\n\n6. Data Pipeline\n\nPre-processing &amp; Cleaning: spam removal, merge user ID, dsb.\nReplicability: manual pre-processing OK di POC, tapi untuk production gunakan tools: TensorFlow Transform, Apache Beam, Airflow.\nMeta-data, Provenance &amp; Lineage:\n\nMetadata: info tentang data (waktu, kamera, labeler ID)\nProvenance: asal data\nLineage: sequence steps pipeline\n\n\nTracking ini penting untuk maintain &amp; debugging pipeline kompleks.\n\n\n7. Balanced Train/Dev/Test Splits\n\nSmall datasets: random split bisa tidak representatif → gunakan balanced split (proporsi positif sama di train/dev/test).\nLarge datasets: random split biasanya cukup representatif.\n\n\n8. Scoping Proyek ML\n\nTujuan: pastikan impact &amp; feasibility.\nProses:\n\nBrainstorm masalah bisnis → solusi AI\nNilai feasibility &amp; value\nTentukan milestones &amp; resource\n\n\nFeasibility: benchmark eksternal, HLP, ketersediaan fitur prediktif, histori proyek.\nValue: hubungkan ML metrics → business metrics (revenue, engagement) via estimasi Fermi.\nEthical considerations: pastikan net positive societal value, fairness, bebas bias.\nMilestones &amp; resourcing: ML metrics, software metrics (latency, throughput), business metrics, resources, timeline. Untuk ketidakpastian → POC atau benchmarking.\n\n\n\nKesimpulan: Kualitas data, baseline performa, HLP, dan scoping proyek adalah fondasi sukses ML production. Iterasi data + model + error analysis + pipeline replicability sangat penting.\n\nLihat Dokumen\nBerikut dokumen bisa dibaca langsung di halaman:\n\n"},"index":{"slug":"index","filePath":"index.md","title":"index","links":[],"tags":[],"content":"Hai, saya Miftah! 👋\nSelamat datang di blog saya, tempat saya membagikan catatan.\n\nSilakan jelajahi artikel-artikel saya, dan semoga bermanfaat!"}}